{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iel3-lQX6bZv"
      },
      "source": [
        "# IBA4315_AS3. Building LLM-powered Applications with APIs (70')\n",
        "ğŸ’ª **Objectives:**\n",
        "- Imagine that you are a developer now. You are asked to build LLM-powered applications by calling API and feeding prompts.\n",
        "- You are encouraged to improve the applications with proper prompting strategies.\n",
        "- When you think the applications can perfectly perform the required tasks, you can stop prompt engineering.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Q1fCZBe-KZ"
      },
      "source": [
        "## Install Packages\n",
        "Install all the necessary packages, it may take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4I00KtDfDs3",
        "outputId": "2dbbc69f-d6f7-4466-ba19-5dd876b92847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.23.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.14.0 (from gradio)\n",
            "  Downloading gradio_client-0.14.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.3.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.3.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.14.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.14.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=f44d4321ea39b0c85ff37aef88ccfe4d2f7df037296a2ab9a859d91260611fba\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.0 ffmpy-0.3.2 gradio-4.23.0 gradio-client-0.14.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 orjson-3.9.15 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.4 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n",
            "Collecting openai\n",
            "  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-1.14.3\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install gradio\n",
        "%pip install openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx4EsADZmzxQ"
      },
      "source": [
        "## Import and Setup\n",
        "**Please make double sure not to share the api with anyone else.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AcOCl6lSU2X",
        "outputId": "4ff21dc7-1d4f-4a5d-e84e-16a91a3d3555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set ChatGPT API sucessfully!!\n"
          ]
        }
      ],
      "source": [
        "# import the packages\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "## Kindly keep the key for yourself and for the assignment only\n",
        "API_SECRET_KEY = \"sk-nWSoC3jZh6LvMSVe30541cAf25B447D9Bf1fFc19239cDcA7\"\n",
        "BASE_URL = \"https://chatapi.onechat.fun/v1\"\n",
        "client = OpenAI(api_key=API_SECRET_KEY, base_url=BASE_URL)\n",
        "\n",
        "# Check if you have set your ChatGPT API successfully\n",
        "# You should see \"Set ChatGPT API sucessfully!!\" if nothing goes wrong.\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages = [{'role':'user','content': \"test\"}],\n",
        "            max_tokens=1,\n",
        "    )\n",
        "    print(\"Set ChatGPT API sucessfully!!\")\n",
        "except:\n",
        "    print(\"There seems to be something wrong with your ChatGPT API. Please follow our demonstration in the slide to get a correct one.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifCr17v4d12K"
      },
      "source": [
        "## Part 1: Review Analyzer (35')  \n",
        "\n",
        "In this task, you are asked to prompt your chatbot into a **review analyzer.**  When the resturant inputs any reviews, it can **analyze the reviews for the resturant.**Your prompts should enable the chatbot to:\n",
        "   \n",
        "ğŸ‘‡***Functional requirements (20')***  \n",
        "*   Count the number of positive and negative reviews\n",
        "*   Extract all the recommended dishes in the given reviews\n",
        "*   Summarize any negative aspects that are mentioned in the review\n",
        "*   Identify the ID of any negative reviews that requires the resturant's immediate reply and generate a reply for the resturant (using right language, no more than 100 words)  \n",
        "\n",
        "ğŸ‘‡***Non-functional requirements (15')***  \n",
        "*   The output should be as short as possible, well organized in a JSON object\n",
        "*   Properly deal with the cases where the relevant information is not present in the given review\n",
        "*   Prevent the users from using the review analyzer for any irrelevant tasks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "a8t6IKgLVQvF",
        "outputId": "11b294bd-7e1b-4b8d-c018-79f57d1874db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f1011e447386f77021.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f1011e447386f77021.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f1011e447386f77021.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## TODO: Design the prompt in English by defining prompt_for_review_analyzer \"\"\n",
        "## TODO: Design the prompt in English by defining prompt_for_review_analyzer \"\"\n",
        "prompt_for_review_analyzer = f\"\"\"\n",
        "Given the following reviews, please:\n",
        "\n",
        "1. Count the number of positive and negative reviews.\n",
        "2. Extract all the recommended dishes in the given reviews.\n",
        "3. Summarize any negative aspects that are mentioned.\n",
        "4. Identify the ID of any negative reviews that requires the restaurant's immediate reply\n",
        "5. generate a complete reply for the restaurant in the language consistent with the negative review.\n",
        "\n",
        "For example:\n",
        "i. Positive reviews: ; Negative reviews:\n",
        "ii. recommended dishes:\n",
        "iii. negative aspects:\n",
        "iv: ID of any negative reviews:\n",
        "V. Repley:\n",
        "\n",
        "Please remeber that the output should be short and well organized in a JSON object.\n",
        "If the relevant information is not present in the given review, use null.\n",
        "If the users is using the review analyzer for any irrelevant tasks, please provide a warning message to the user.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Function to reset the conversation\n",
        "def reset() -> List:\n",
        "    return []\n",
        "\n",
        "# Function to call the model to generate\n",
        "def interact_review_analyzer(prompt: str, reviews: str, temp = 1.0) -> List[Tuple[str, str]]:\n",
        "    '''\n",
        "    * Arguments\n",
        "\n",
        "      - prompt: the prompt that we use in this section\n",
        "\n",
        "      - reviews: the reviews to be analyzed\n",
        "\n",
        "      - temp: the temperature parameter of this model. Temperature is used to control the output of the chatbot.\n",
        "              The higher the temperature is, the more creative response you will get.\n",
        "\n",
        "    '''\n",
        "    input = f\"{prompt}\\n{reviews}\"\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages = [{'role':'user','content': input}],\n",
        "            temperature = temp,\n",
        "            max_tokens=200,\n",
        "    )\n",
        "\n",
        "    return [(input, response.choices[0].message.content)]\n",
        "\n",
        "# Function to export the whole conversation\n",
        "def export_review_analyzer(chatbot: List[Tuple[str, str]], reviews: str) -> None:\n",
        "    '''\n",
        "    * Arguments\n",
        "\n",
        "      - chatbot: the model itself, the conversation is stored in list of tuples\n",
        "\n",
        "      - reviews: the reviews to be analyzed\n",
        "\n",
        "    '''\n",
        "    target = {\"chatbot\": chatbot, \"reviews\": reviews}\n",
        "    with open(\"Part1.json\", \"w\") as file:\n",
        "        json.dump(target, file)\n",
        "\n",
        "# This part generates the Gradio UI interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Part1: Review_analyzer\\nFill in any reviews and let the chatbot analyze it for you!!\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    prompt_textbox = gr.Textbox(label=\"Prompt\", value=prompt_for_review_analyzer, visible=False)\n",
        "    review_textbox = gr.Textbox(label=\"Reviews\", interactive = True, value = f\"\"\"####\n",
        "1. æ¸…çœŸæ‰‹æŠ“ç¾Šè‚‰é¥­ï¼Œçœ‹èµ·æ¥ä¸é”™ï¼Œä½†æ˜¯åƒåˆ°å˜´é‡Œå‘ç°è¿™é‡Œé¢æ°´åˆ†å¤ªå¤šï¼Œ\\\n",
        "åŸºæœ¬æ²¡æœ‰ç¾Šè‚‰ä»€ä¹ˆäº‹ï¼Œå¦¥å¦¥æˆä¸ºèƒ¡èåœæ‹Œé¥­ï¼Œæ±¤ä¹Ÿå’¸ï¼Œè¯´æ˜ä»–ä»¬æ”¾çš„ç¾Šè‚‰ä¸å¤Ÿï¼Œ\\\n",
        "ç¾Šæ²¹ä¸å¤Ÿåä»¥æ°´ä»£ä¹‹æ¥ç…®ï¼Œä½¿å¾—æ•´ä¸ªæ‰‹æŠ“é¥­å°±è½¯ç»µç»µä¸€å›¢ç”œç±³é¥­ï¼Œå¸Œæœ›æ—¥åèƒ½æ”¹ä¸€æ”¹ï¼Œæ—¥åå†è§ã€‚\\n\n",
        "####\n",
        "2. ä½œä¸ºæ¸¯ä¸­æ·±çš„ä¸‹å›­é£Ÿå ‚ï¼Œå…¥é©»çš„å•†å®¶è›®å¤šçš„ï¼Œèœä¹Ÿæ™®éè¿˜ä¸é”™ï¼Œå®Œèƒœé«˜ä¸­é£Ÿå ‚ã€‚\\\n",
        "æœ¬äººæ¯”è¾ƒå–œæ¬¢é‡åº†å°é¢çª—å£çš„é‡åº†å°é¢å’Œç‡ƒé¢ï¼Œå£å‘³é¦™è¾£ï¼Œåƒèµ·æ¥å¾ˆçˆ½ã€‚\\\n",
        "éº»è¾£çƒ«çª—å£ä¹Ÿä¸é”™ï¼Œå¯ä»¥è‡ªåŠ©é€‰èœï¼Œè‡ªåŠ©åŠ éº»æ±ã€é†‹ç­‰å°æ–™è°ƒå‘³ã€‚\\\n",
        "è‡ªé€‰èœçª—å£ä»·æ ¼è¾ƒè´µ,ä¸æ¨èã€‚ä¹¦äº¦çƒ§ä»™è‰çš„èŒ‰é¦™å¥¶ç»¿çœŸçš„å¥½å–ï¼\\n\n",
        "####\n",
        "3. é›¨ç”°ç²¿æ¡å¤ªå–œæ¬¢å•¦ï¼ä»½é‡è¶³ï¼Œæ–°é²œç¾å‘³ï¼Œå¤§éª¨æ±¤ç°åœºåœ¨ç†¬ç€ï¼Œå–èµ·æ¥å¾ˆèˆ’æœï¼Œæœ‰å‘³ï¼\\\n",
        "çŒªæ‚æ˜¯é²œè‚‰ï¼ŒçŒªå¿ƒï¼Œç²‰è‚ ï¼Œè‚‰é¥¼ç­‰ï¼Œä¸°å¯Œï¼Œä¸¤ç‰‡èœå¶å­ä¹Ÿæ–°é²œã€‚ä»·æ ¼ä¹Ÿå®æƒ ï¼Œä¸€ä»½17\\\n",
        "ä¹Ÿæœ‰ç‰›è‚‰å¯é€‰ï¼Œé…±æ–™ä¹Ÿæœ‰æ²™èŒ¶é…±å“‡ï¼å«ç”Ÿå¥åº·ä¹‹é€‰ã€‚ç”¨é¤å¾ˆæ„‰å¿«ï¼Œä¿æŒå–”\"\"\")\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"#  Temperature\\n Temperature is used to control the output of the chatbot. The higher the temperature is, the more creative response you will get.\")\n",
        "        temperature_slider = gr.Slider(0.0, 2.0, 1.0, step = 0.1, label=\"Temperature\")\n",
        "    with gr.Row():\n",
        "        sent_button = gr.Button(value=\"Send\")\n",
        "        reset_button = gr.Button(value=\"Reset\")\n",
        "\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"#  Save your Result.\\n After you get a satisfied result. Click the export button to recode it.\")\n",
        "        export_button = gr.Button(value=\"Export\")\n",
        "    sent_button.click(interact_review_analyzer, inputs=[prompt_textbox, review_textbox, temperature_slider], outputs=[chatbot])\n",
        "    reset_button.click(reset, outputs=[chatbot])\n",
        "    export_button.click(export_review_analyzer, inputs=[chatbot, review_textbox])\n",
        "\n",
        "\n",
        "demo.launch(debug = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTrh4gK6ZdgN"
      },
      "source": [
        "## Part 2: IBA4315_bot (35')\n",
        "In Part 2, you are asked to prompt a chatbot for the course IBA4315. Interact with the bot for **3 rounds** by typing (ir)relevant questions in the Input field to **demonstrate that your bot is capable of** answering common questions related to course syllabus, assignment deadlines, quiz time, grading components, general policy. **Indicate any principles you considered when designing and refining your prompts.**\n",
        "\n",
        "\n",
        "You need to:  \n",
        "1. Come up with a prompt for **IBA4315_bot** and fill it in **prompt_for_faqbot**.\n",
        "`ğŸ‘‡The course information you may need is provided in the code cell.`\n",
        "1. **Hit the run button.** An interface wiill pop up.\n",
        "2. **Click the public or local URL** and open the chatbot in a seperate window of your browser. Type your question in the Input field.\n",
        "3. Hit the \"Send\" button to produce the results. (You can use the \"Temperature\" slide to control the creativeness of the output.)\n",
        "4. If you **want to change your prompt**, hit the run button again to stop the cell. Then go back to step 1.\n",
        "5. **After you get the desired result**, create the **screen shot in the seperate window** and hit the button **\"Export\"** to save your result. There will be a file named **part2.json** appearing in the file list.\n",
        "\n",
        "**â—Important Note:â—**\n",
        "\n",
        "*  **If you hit the \"Export\" button again, the previous result will be covered.**\n",
        "*  **You should keep in mind that even with the exact same prompt, the output might still differ.**\n",
        "*  **Remember to stop this cell before you go on to the next one.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPqyNAyiKKTW"
      },
      "source": [
        "### Please indicate what principles or techniques you incorporate in `prompt_for_faqbot`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "lBWjkrXbZgfL",
        "outputId": "5abc4c0e-2637-4a77-9069-5682c94f6bf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://8bbbef0ee840ca588e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://8bbbef0ee840ca588e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8bbbef0ee840ca588e.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Design the prompt in English by defining prompt_for_faqbot\"\"\n",
        "chatbot_task = \"Answer common questions about course syllabus, \\\n",
        "assignment deadlines, quiz time, grading components, general policy of IBA4315\"\n",
        "\n",
        "prompt_for_faqbot = f\"\"\"\n",
        "\n",
        "You are a smart course assistant, please be polite and interactive. You should be able to answer any questions about the IBA4315 course, such as:\n",
        "\n",
        "- What is the grading component of the course?\n",
        "- When are the assignment deadlines?\n",
        "- When are the quiz times?\n",
        "- What is the general policy of the course?\n",
        "- Tell me the syllabus of the course?\n",
        "\n",
        "\n",
        "The information you may refer to is shown as follow:\n",
        "##Grading Component##\n",
        "Participation - 10%\n",
        "Assignment (4 coding and 1 case analysis) - 38%\n",
        "In-class quiz (3 times) - 12%\n",
        "Term project (40%)\n",
        "\n",
        "##General Policy##\n",
        "Use of AI Policy: Directly coping GPT-generated answers is not allowed (plagiarism)\n",
        "No Late Policy: For each type of submissions (i.e., assignments and project),\\\n",
        "you can have a one-day extension without penalty, but only one time for each\n",
        "Penalty is 20% deduction of your grade for each additional late day.\n",
        "Collaboration Policy: Free-rider not allowed!\n",
        "\n",
        "##syllabus##\n",
        "Jan 09 & 11\t- Course overview and Intro to AI, ML, DL\n",
        "Jan 16 & 18\t- Acquiring data for AI/ML solutions\n",
        "Jan 23 & 25\t- Neural networks (I) - Basics\n",
        "Jan 30 & Feb 01\t- Neural networks (II) - Python Demonstrations & Optimizations\n",
        "Feb 27 & 29\t- Neural networks (III) - MLP&CNN; On-Device and Cloud AI\n",
        "Mar 05 & 07\t- Intro to Gen AI & LLMs; Prompt Engineering (I)\n",
        "Mar 12 & 14\t- Prompt Engineering(II) & OpenAI API\n",
        "Mar 19 & 21\t- Prompt Engineering(III) & LLM Techniques (I)\n",
        "Mar 26 & 28\t- LLM Techniques (II)\n",
        "Apr 02 & 07\t- Image generation model\n",
        "Apr 09 & 11 - AI Agent and other trending use cases of LLMs\n",
        "Apr 16 & 18 - Case study: DBS BANK; Managing AI in organizations\n",
        "Apr 23 & 25 - Invited lecture by guest speaker; Term project presentation 1\n",
        "Apr 30 - Term project presentation 2\n",
        "\n",
        "##Important Timeline##\n",
        "Assignment:\n",
        "AS1, Noon on Jan 31 2024\n",
        "AS2, Noon on Mar 06 2024\n",
        "AS3, Noon on Mar 27 2024\n",
        "AS4, Noon on Apr 03 2024\n",
        "Case, Noon on Apr 21 2024\n",
        "Quiz:\n",
        "Quiz1, Feb 01 2024\n",
        "Quiz2, Mar 21 2024\n",
        "Quiz3, Apr 11 2024\n",
        "Project:\n",
        "Project proposal, Noon on Mar 13 2024\n",
        "Project presentation session 1, Mar 25 2024\n",
        "Project presentation session 1, Mar 30 2024\n",
        "Project report, Noon on May 12 2024\n",
        "\"\"\"\n",
        "\n",
        "# Function to clear the conversation\n",
        "def reset() -> List:\n",
        "    return []\n",
        "\n",
        "# Function to call the model to generate\n",
        "def interact_customize(chatbot: List[Tuple[str, str]], prompt: str ,user_input: str, temperature = 1.0) -> List[Tuple[str, str]]:\n",
        "    '''\n",
        "    * Arguments\n",
        "\n",
        "      - chatbot: the model itself, the conversation is stored in list of tuples\n",
        "\n",
        "      - prompt: the prompt for your desginated task\n",
        "\n",
        "      - user_input: the user input of each round of conversation\n",
        "\n",
        "      - temp: the temperature parameter of this model. Temperature is used to control the output of the chatbot.\n",
        "              The higher the temperature is, the more creative response you will get.\n",
        "\n",
        "    '''\n",
        "    try:\n",
        "        messages = []\n",
        "        messages.append({'role': 'user', 'content': prompt})\n",
        "        for input_text, response_text in chatbot:\n",
        "            messages.append({'role': 'user', 'content': input_text})\n",
        "            messages.append({'role': 'assistant', 'content': response_text})\n",
        "\n",
        "        messages.append({'role': 'user', 'content': user_input})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages = messages,\n",
        "            temperature = temperature,\n",
        "            max_tokens=200,\n",
        "        )\n",
        "\n",
        "        chatbot.append((user_input, response.choices[0].message.content))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        chatbot.append((user_input, f\"Sorry, an error occurred: {e}\"))\n",
        "    return chatbot\n",
        "\n",
        "# Function to export the whole conversation log\n",
        "def export_customized(chatbot: List[Tuple[str, str]], description: str) -> None:\n",
        "    '''\n",
        "    * Arguments\n",
        "\n",
        "      - chatbot: the model itself, the conversation is stored in list of tuples\n",
        "\n",
        "      - description: the description of this task\n",
        "\n",
        "    '''\n",
        "    target = {\"chatbot\": chatbot, \"description\": description}\n",
        "    with open(\"part2.json\", \"w\") as file:\n",
        "        json.dump(target, file)\n",
        "\n",
        "# This part constructs the Gradio UI interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Part2: IBA4315_bot\\nThe chatbot is able to answer common questions about course syllabus, \\\n",
        "    assignment deadlines, quiz time, grading components, general policy of IBA4315. Try to interact with it!!\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    desc_textbox = gr.Textbox(label=\"Description of the task\", value=chatbot_task, interactive=False)\n",
        "    prompt_textbox = gr.Textbox(label=\"Prompt\", value=prompt_for_faqbot, visible=False)\n",
        "    input_textbox = gr.Textbox(label=\"Input\")\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"#  Temperature\\n Temperature is used to control the output of the chatbot. The higher the temperature is, the more creative response you will get.\")\n",
        "        temperature_slider = gr.Slider(0.0, 2.0, 1.0, step = 0.1, label=\"Temperature\")\n",
        "    with gr.Row():\n",
        "        sent_button = gr.Button(value=\"Send\")\n",
        "        reset_button = gr.Button(value=\"Reset\")\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"#  Save your Result.\\n After you get a satisfied result. Click the export button to recode it.\")\n",
        "        export_button = gr.Button(value=\"Export\")\n",
        "    sent_button.click(interact_customize, inputs=[chatbot, prompt_textbox, input_textbox, temperature_slider], outputs=[chatbot])\n",
        "    reset_button.click(reset, outputs=[chatbot])\n",
        "    export_button.click(export_customized, inputs=[chatbot, desc_textbox])\n",
        "\n",
        "demo.launch(debug = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4e4K-1zxrd8"
      },
      "source": [
        "\n",
        "\n",
        "1. Clarity: clearly define the role and task of the model to guide it towards generating the correct responses.\n",
        "\n",
        "2. Context: Where possible, the prompt should provide sufficient context to help the model better understand the question.\n",
        "\n",
        "3. Conciseness: The prompt should be as concise as possible, avoiding unnecessary or irrelevant information.\n",
        "\n",
        "4. User Engagement: The prompt encourages users to interact with the bot, enhancing user engagement.\n",
        "\n",
        "5. Avoid negative sentences,try to directly tell the bot what is needed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iel3-lQX6bZv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
